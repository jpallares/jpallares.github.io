

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Designing Data-Intensive Applications - Juan Pallarès Garbí</title>







<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Juan Pallarès Garbí">
<meta property="og:title" content="Designing Data-Intensive Applications">


  <link rel="canonical" href="https://juan.pallares.me/_books/designing-data-intensive-applications/">
  <meta property="og:url" content="https://juan.pallares.me/_books/designing-data-intensive-applications/">





  <meta name="twitter:site" content="@juanpallares">
  <meta name="twitter:title" content="Designing Data-Intensive Applications">
  <meta name="twitter:description" content="Landing page.">
  <meta name="twitter:url" content="https://juan.pallares.me/_books/designing-data-intensive-applications/">

  
    <meta name="twitter:card" content="summary">
    
      <meta name="twitter:image" content="https://juan.pallares.me/images/Untitled.png">
    
  

  



  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-03-02T00:00:00+01:00">
  
  



  <script type="application/ld+json">
    {
      "@context": "http://schema.org",
      "@type": "Organization",
      "url": "https://juan.pallares.me",
      "logo": "https://juan.pallares.me/images/Untitled.png"
    }
  </script>



  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "person",
      "name" : "Juan Pallarès Garbí",
      "url" : "https://juan.pallares.me",
      "sameAs" : ["https://twitter.com/juanpallares","https://www.linkedin.com/in/juanpallares"]
    }
  </script>






<!-- end SEO -->

<link href="https://juan.pallares.me/feed.xml" type="application/atom+xml" rel="alternate" title="Juan Pallarès Garbí Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://juan.pallares.me/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    <!-- start custom head snippets -->

<!-- insert favicons. use http://realfavicongenerator.net/ -->

<!-- end custom head snippets -->
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://juan.pallares.me/">Juan Pallarès Garbí</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://juan.pallares.me/cv/">cv</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://juan.pallares.me/books/">books</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://juan.pallares.me/images/juanpallares.jpg" class="author__avatar" alt="Juan Pallarès">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Juan Pallarès</h3>
    <p class="author__bio">Software Engineer, football and technology lover from Barcelona.</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Barcelona</li>
      
      
      
        <li><a href="mailto:juan@pallares.me"><i class="fa fa-fw fa-envelope-square" aria-hidden="true"></i> </a></li>
      
      
      
      
      
        <li><a href="https://www.linkedin.com/in/juanpallares"><i class="fa fa-fw fa-linkedin-square" aria-hidden="true"></i> LinkedIn</a></li>
      
      
      
      
      
      
        <li><a href="https://github.com/jpallares"><i class="fa fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
        <li><a href="https://gitlab.com/jpallares"><i class="fa fa-fw fa-gitlab" aria-hidden="true"></i> Gitlab</a></li>
      
      
        <li><a href="https://twitter.com/juanpallares"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
        <li><a href="https://www.stackoverflow.com/users/https://stackoverflow.com/users/500843/juan"><i class="fa fa-fw fa-stack-overflow" aria-hidden="true"></i> Stackoverflow</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Designing Data-Intensive Applications">
    
    <meta itemprop="datePublished" content="March 02, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Designing Data-Intensive Applications
</h1>
          
        </header>
      

      

      <section class="page__content" itemprop="text">
        <h2 id="kleppmann-martin"><em>Kleppmann, Martin</em></h2>

<h4 id="march-02-2021">March 02, 2021</h4>

<p>By structuring applications around dataflow and checking constraints asynchronously, we can avoid most coordination and create systems that maintain integrity but still perform well, even in geographically distributed scenarios and in the presence of faults.</p>

<h4 id="march-02-2021-1">March 02, 2021</h4>

<p>We discussed how to solve this data integration problem by using batch processing and event streams to let data changes flow between different systems. In this approach, certain systems are designated as systems of record, and other data is derived from them through transformations. In this way we can maintain indexes, materialized views, machine learning models, statistical summaries, and more. By making these derivations and transformations asynchronous and loosely coupled, a problem in one area is prevented from spreading to unrelated parts of the system, increasing the robustness and fault-tolerance of the system as a whole.</p>

<h4 id="march-02-2021-2">March 02, 2021</h4>

<p>We discussed how to solve this data integration problem by using batch processing and event streams to let data changes flow between different systems.</p>

<h4 id="march-02-2021-3">March 02, 2021</h4>

<p>environmental challenge. Almost all computers produce information. It stays around, festering. How we deal with it—how we contain it and how we dispose of it—is central to the health of our information economy. Just as we look back today at the early decades of the industrial age and wonder how our ancestors could have ignored pollution in their rush to build an industrial world, our grandchildren will look back at us during these early decades of the information age and judge us on how we addressed the challenge of data collection and misuse. We should try to make them proud.</p>

<h4 id="march-02-2021-4">March 02, 2021</h4>

<p>In the words of Bruce Schneier [96]: Data is the pollution problem of the information age, and protecting privacy is the</p>

<h4 id="march-02-2021-5">March 02, 2021</h4>

<p>In the words of Bruce Schneier [96]: Data is the pollution problem of the information age, and protecting privacy is the</p>

<h4 id="march-02-2021-6">March 02, 2021</h4>

<p>In the words of Bruce Schneier [96]: Data is the pollution problem of the information age, and protecting privacy is the</p>

<h4 id="march-01-2021">March 01, 2021</h4>

<p>When services become good at predicting what content users want to see, they may end up showing people only opinions they already agree with, leading to echo chambers in which stereotypes, misinformation, and polarization can breed.</p>

<h4 id="march-01-2021-1">March 01, 2021</h4>

<p>Another way of looking at coordination and constraints: they reduce the number of apologies you have to make due to inconsistencies, but potentially also reduce the performance and availability of your system, and thus potentially increase the number of apologies you have to make due to outages. You cannot reduce the number of apologies to zero, but you can aim to find the best trade-off for your needs—the sweet spot where there are neither too many inconsistencies nor too many availability problems.</p>

<h4 id="march-01-2021-2">March 01, 2021</h4>

<p>these observations mean that dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees. Such coordination-avoiding data systems have a lot of appeal: they can achieve better performance and fault tolerance than systems that need to perform synchronous coordination</p>

<h4 id="march-01-2021-3">March 01, 2021</h4>

<p>These applications do require integrity: you would not want to lose a reservation, or have money disappear due to mismatched credits and debits. But they don’t require timeliness on the enforcement of the constraint, if you have sold more items than you have in the warehouse, you can patch up the problem after the fact by apologizing.</p>

<h4 id="march-01-2021-4">March 01, 2021</h4>

<p>an interesting property of the event-based dataflow systems that we have discussed in this chapter is that they decouple timeliness and integrity.</p>

<h4 id="march-01-2021-5">March 01, 2021</h4>

<p>reliable stream processing systems can preserve integrity without requiring distributed transactions and an atomic commit protocol, which means they can potentially achieve comparable correctness with much better performance and operational robustness.</p>

<h4 id="march-01-2021-6">March 01, 2021</h4>

<p>in most applications, integrity is much more important than timeliness. Violations of timeliness can be annoying and confusing, but violations of integrity can be catastrophic.</p>

<h4 id="march-01-2021-7">March 01, 2021</h4>

<p>In slogan form: violations of timeliness are “eventual consistency,” whereas violations of integrity are “perpetual inconsistency.”</p>

<h4 id="february-28-2021">February 28, 2021</h4>

<p>The end-to-end argument also applies to checking the integrity of data: checksums built into Ethernet, TCP, and TLS can detect corruption of packets in the network, but they cannot detect corruption due to bugs in the software at the sending and receiving ends of the network connection, or corruption on the disks where the data is stored. If you want to catch all possible sources of data corruption, you also need end-to-end checksums. A similar argument applies with encryption - the password on your home WiFi network protects against people snooping your WiFi traffic, but not against attackers elsewhere on the internet; TLS/SSL between your client and the server protects against network attackers, but not against compromises of the server. Only end-to-end encryption and authentication can protect against all of these things.</p>

<h4 id="february-28-2021-1">February 28, 2021</h4>

<p>By themselves, TCP, database transactions, and stream processors cannot entirely rule out these duplicates. Solving the problem requires an end-to-end solution: a transaction identifier that is passed all the way from the end-user client to the database.</p>

<h4 id="february-28-2021-2">February 28, 2021</h4>

<p>building for scale that you don’t need is wasted effort and may lock you into an inflexible design. In effect, it is a form of premature optimization.</p>

<h4 id="february-28-2021-3">February 28, 2021</h4>

<p>In the absence of widespread support for a good distributed transaction protocol, I believe that log-based derived data is the most promising approach for integrating different data systems. However, guarantees such as reading your own writes are useful, and I don’t think that it is productive to tell everyone “eventual consistency is inevitable—suck it up and learn to deal with it” (at least not without good guidance on how to deal with it).</p>

<h4 id="february-28-2021-4">February 28, 2021</h4>

<p>If it is possible for you to funnel all user input through a single system that decides on an ordering for all writes, it becomes much easier to derive other representations of the data by processing the writes in the same order.</p>

<h4 id="february-28-2021-5">February 28, 2021</h4>

<p>Faced with this profusion of alternatives, the first challenge is then to figure out the mapping between the software products and the circumstances in which they are a good fit.</p>

<h4 id="february-27-2021">February 27, 2021</h4>

<p>If the highest aim of a captain was the preserve his ship, he would keep it in port forever.)</p>

<h4 id="february-26-2021">February 26, 2021</h4>

<p>The boundary between CEP and stream analytics is blurry, but as a general rule, analytics tends to be less interested in finding specific event sequences and is more oriented toward aggregations and statistical metrics over a large number of events—for</p>

<h4 id="february-26-2021-1">February 26, 2021</h4>

<p>CEP engines reverse these roles: queries are stored long-term, and events from the input streams continuously flow past them in search of a query that matches an event pattern [68].</p>

<h4 id="february-26-2021-2">February 26, 2021</h4>

<p>Complex event processing Complex event processing (CEP) is an approach developed in the 1990s for analyzing event streams, especially geared toward the kind of application that requires searching for certain event patterns [65, 66]. Similarly to the way that a regular expression allows you to search for certain patterns of characters in a string, CEP allows you to specify rules to search for certain patterns of events in a stream.</p>

<h4 id="february-24-2021">February 24, 2021</h4>

<p>Transaction logs record all the changes made to the database. High-speed appends are the only way to change the log. From this perspective, the contents of the database hold a caching of the latest record values in the logs. The truth is the log. The database is a cache of a subset of the log. That cached subset happens to be the latest value of each record and index value from the log.</p>

<h4 id="february-23-2021">February 23, 2021</h4>

<p>Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems so that the derived systems have an accurate copy of the data.</p>

<h4 id="february-23-2021-1">February 23, 2021</h4>

<p>Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems so that the derived</p>

<h4 id="february-22-2021">February 22, 2021</h4>

<p>Why can we not have a hybrid, combining the durable storage approach of databases with the low-latency notification facilities of messaging? This is the idea behind log-based message brokers.</p>

<h4 id="february-22-2021-1">February 22, 2021</h4>

<p>A widely used alternative is to send messages via a message broker (also known as a message queue), which is essentially a kind of database that is optimized for handling message streams [13]. It runs as a server, with producers and consumers connecting to it as clients.</p>

<h4 id="february-21-2021">February 21, 2021</h4>

<p>This architecture allows non-production (low-priority) computing resources to be overcommitted, because the system knows that it can reclaim the resources if necessary. Overcommitting resources in turn allows better utilization of machines and greater efficiency compared to systems that segregate production and non-production tasks.</p>

<h4 id="february-21-2021-1">February 21, 2021</h4>

<p>This approach has been dubbed the sushi principle: “raw data is better”</p>

<h4 id="february-21-2021-2">February 21, 2021</h4>

<p>The idea is similar to a data warehouse (see “Data Warehousing”): simply bringing data from various parts of a large organization together in one place is valuable, because it enables joins across datasets that were previously disparate. The careful schema design required by an MPP database slows down that centralized data collection; collecting data in its raw form, and worrying about schema design later, allows the data collection to be speeded up (a concept sometimes known as a “data lake” or “enterprise data hub” [55]).</p>

<h4 id="february-21-2021-3">February 21, 2021</h4>

<p>To put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data into HDFS, and only later figuring out how to process it further [53]. By contrast, MPP databases typically require careful up-front modeling of the data and query patterns before importing the data into the database’s proprietary storage format.</p>

<h4 id="february-21-2021-4">February 21, 2021</h4>

<p>genome sequences, or any other kind of data. To put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data into HDFS, and only later figuring out how to process it further [53]. By contrast, MPP databases typically require careful up-front modeling of the data and query patterns before importing the data into the database’s proprietary storage format.</p>

<h4 id="february-21-2021-5">February 21, 2021</h4>

<p>Another common use for grouping is collating all the activity events for a particular user session, in order to find out the sequence of actions that the user took—a process called sessionization [37]. For example, such analysis could be used to work out whether users who were shown a new version of your website are more likely to make a purchase than those who were shown the old version (A/B testing), or to calculate whether some marketing activity is worthwhile.</p>

<h4 id="february-18-2021">February 18, 2021</h4>

<p>Like a batch processing system, a stream processor consumes inputs and produces outputs (rather than responding to requests). However, a stream job operates on events shortly after they happen, whereas a batch job operates on a fixed set of input data. This difference allows stream processing systems to have lower latency than the equivalent batch systems.</p>

<h4 id="february-01-2021">February 01, 2021</h4>

<p>the example of ensuring that a username is unique and rejecting concurrent registrations for the same username. If one node is going to accept a registration, it needs to somehow know that another node isn’t concurrently in the process of registering the same name. This problem led us toward consensus.</p>

<h4 id="february-01-2021-1">February 01, 2021</h4>

<p>causality, which imposes an ordering on events in a system (what happened before what, based on cause and effect). Unlike linearizability, which puts all operations in a single, totally ordered timeline, causality provides us with a weaker consistency model: some things can be concurrent, so the version history is like a timeline with branching and merging. Causal consistency does not have the coordination overhead of linearizability and is much less sensitive to network problems.</p>

<h4 id="february-01-2021-2">February 01, 2021</h4>

<p>linearizability, a popular consistency model: its goal is to make replicated data appear as though there were only a single copy, and to make all operations act on it</p>

<h4 id="february-01-2021-3">February 01, 2021</h4>

<p>Every time the current leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number, and thus epoch numbers are totally ordered and monotonically increasing. If there is a conflict between two different leaders in two different epochs (perhaps because the previous leader actually wasn’t dead after all), then the leader with the higher epoch number prevails.</p>

<h4 id="february-01-2021-4">February 01, 2021</h4>

<p>It seems that in order to elect a leader, we first need a leader. In order to solve consensus, we must first solve consensus. How do we break out of this conundrum?</p>

<h4 id="january-31-2021">January 31, 2021</h4>

<p>However, most implementations of consensus ensure that the safety properties—agreement, integrity, and validity—are always met, even if a majority of nodes fail or there is a severe network problem [92]. Thus, a large-scale outage can stop the system from being able to process requests, but it cannot corrupt the consensus system by causing it to make invalid decisions.</p>

<h4 id="january-31-2021-1">January 31, 2021</h4>

<p>The validity property exists mostly to rule out trivial solutions: for example, you could have an algorithm that always decides null, no matter what was proposed; this algorithm would satisfy the agreement and integrity properties, but not the validity property.</p>

<h4 id="january-28-2021">January 28, 2021</h4>

<p>DON’T CONFUSE 2PC AND 2PL Two-phase commit (2PC) and two-phase locking (see “Two-Phase Locking (2PL)”) are two very different things. 2PC provides atomic commit in a distributed database, whereas 2PL provides serializable isolation. To avoid confusion, it’s best to think of them as entirely separate concepts and to ignore the unfortunate similarity in the names.</p>

<h4 id="january-27-2021">January 27, 2021</h4>

<p>it can be proved that a linearizable compare-and-set (or increment-and-get) register and total order broadcast are both equivalent to consensus [28, 67]. That is, if you can solve one of these problems, you can transform it into a solution for the others. This is quite a profound and surprising insight!</p>

<h4 id="january-24-2021">January 24, 2021</h4>

<p>causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures</p>

<h4 id="january-24-2021-1">January 24, 2021</h4>

<p>In a linearizable system, we have a total order of operations:</p>

<h4 id="january-24-2021-2">January 24, 2021</h4>

<p>they are concurrent. This means that causality defines a partial order, not a total order: some operations are ordered with respect to each other, but some are incomparable.</p>

<h4 id="january-24-2021-3">January 24, 2021</h4>

<p>Linearizability is slow—and this is true all the time, not only during a network fault.</p>

<h4 id="january-24-2021-4">January 24, 2021</h4>

<p>The CAP theorem as formally defined [30] is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault (network partitions,vi or nodes that are alive but disconnected from each other). It doesn’t say anything about network delays, dead nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little practical value for designing systems</p>

<h4 id="january-24-2021-5">January 24, 2021</h4>

<p>applications that don’t require linearizability can be more tolerant of network problems. This insight is popularly known as the CAP theorem</p>

<h4 id="january-24-2021-6">January 24, 2021</h4>

<p>If your application does not require linearizability, then it can be written in a way that each replica can process requests independently, even if it is disconnected from other replicas (e.g., multi-leader). In this case, the application can remain available in the face of a network problem, but its behavior is not linearizable.</p>

<h4 id="january-24-2021-7">January 24, 2021</h4>

<p>your application does not require linearizability, then it can be written in a way that each replica can process requests independently, even if it is disconnected from other replicas (e.g., multi-leader). In this case, the application can remain available in the face of a network problem, but its behavior is not linearizable.</p>

<h4 id="january-24-2021-8">January 24, 2021</h4>

<p>If your application requires linearizability, and some replicas are disconnected from the other replicas due to a network problem, then some replicas cannot process requests while they are disconnected: they must either wait until the network problem is fixed, or return an error (either way, they become unavailable).</p>

<h4 id="january-24-2021-9">January 24, 2021</h4>

<p>your application requires linearizability, and some replicas are disconnected from the other replicas due to a network problem, then some replicas cannot process requests while they are disconnected: they must either wait until the network problem is fixed, or return an error (either way, they become unavailable).</p>

<h4 id="january-23-2021">January 23, 2021</h4>

<p>A database may provide both serializability and linearizability, and this combination is known as strict serializability or strong one-copy serializability (strong-1SR)</p>

<h4 id="january-23-2021-1">January 23, 2021</h4>

<p>Linearizability is a recency guarantee on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew</p>

<h4 id="january-23-2021-2">January 23, 2021</h4>

<p>Linearizability Linearizability is a recency guarantee on reads and writes of a register (an individual object). It doesn’t group operations together into transactions, so it does not prevent problems such as write skew</p>

<h4 id="january-23-2021-3">January 23, 2021</h4>

<p>It guarantees that transactions behave the same as if they had executed in some serial order (each transaction running to completion before the next transaction starts).</p>

<h4 id="january-23-2021-4">January 23, 2021</h4>

<p>In a linearizable system, as soon as one client successfully completes a write, all clients reading from the database must be able to see the value just written.</p>

<h4 id="january-23-2021-5">January 23, 2021</h4>

<p>transaction isolation is primarily about avoiding race conditions due to concurrently executing transactions, whereas distributed consistency is mostly about coordinating the state of replicas in the face of delays and faults.</p>

<h4 id="january-23-2021-6">January 23, 2021</h4>

<p>The best way of building fault-tolerant systems is to find some general-purpose abstractions with useful guarantees, implement them once, and then let applications rely on those guarantees. This is the same approach as we used with transactions in Chapter 7: by using a transaction, the application can pretend that there are no crashes (atomicity), that nobody else is concurrently accessing the database (isolation), and that storage devices are perfectly reliable (durability).</p>

<h4 id="january-23-2021-7">January 23, 2021</h4>

<p>scalability is not the only reason for wanting to use a distributed system. Fault tolerance and low latency (by placing data geographically close to users) are equally important goals, and those things cannot be achieved with a single node.</p>

<h4 id="january-22-2021">January 22, 2021</h4>

<p>Safety is often informally defined as nothing bad happens, and liveness as something good eventually happens.</p>

<h4 id="january-22-2021-1">January 22, 2021</h4>

<p>Safety is often informally defined as nothing bad happens,</p>

<h4 id="january-21-2021">January 21, 2021</h4>

<p>we have explored the ways in which distributed systems are different from programs running on a single computer: there is no shared memory, only message passing via an unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks, and processing pauses.</p>

<h4 id="january-21-2021-1">January 21, 2021</h4>

<p>When writing multi-threaded code on a single machine, we have fairly good tools for making it thread-safe: mutexes, semaphores, atomic counters, lock-free data structures, blocking queues, and so on. Unfortunately, these tools don’t directly translate to distributed systems, because a distributed system has no shared memory—only messages sent over an unreliable network.</p>

<h4 id="january-19-2021">January 19, 2021</h4>

<p>it’s important to be aware that the definition of “recent” depends on a local time-of-day clock, which may well be incorrect.</p>

<h4 id="january-19-2021-1">January 19, 2021</h4>

<p>its quartz clock is defective or its NTP client is misconfigured, most things will seem to work fine, even though its clock gradually drifts further and further away from reality. If some piece of software is relying on an accurately synchronized clock, the result is more likely to be silent and subtle data loss than a dramatic crash [53, 54].</p>

<h4 id="january-19-2021-2">January 19, 2021</h4>

<p>The same is true with clocks: although they work quite well most of the time, robust software needs to be prepared to deal with incorrect clocks.</p>

<h4 id="january-17-2021">January 17, 2021</h4>

<p>The resolution of monotonic clocks is usually quite good: on most systems they can measure time intervals in microseconds or less.</p>

<h4 id="january-17-2021-1">January 17, 2021</h4>

<p>elapsed between the two checks. However, the absolute value of the clock is meaningless: it might be the number of nanoseconds since the computer was started, or something similarly arbitrary.</p>

<h4 id="january-17-2021-2">January 17, 2021</h4>

<p>Monotonic Versus Time-of-Day Clocks Modern computers have at least two different kinds of clocks: a time-of-day clock and a monotonic clock. Although they both measure time, it is important to distinguish the two, since they serve different purposes.</p>

<h4 id="january-17-2021-3">January 17, 2021</h4>

<p>The name comes from the fact that they are guaranteed to always move forward (whereas a time-of-day clock may jump back in time).</p>

<h4 id="january-17-2021-4">January 17, 2021</h4>

<p>monotonic clock is suitable for measuring a duration (time interval), such as a timeout or a service’s response time:</p>

<h4 id="january-17-2021-5">January 17, 2021</h4>

<p>Monotonic clocks A monotonic clock is suitable for measuring a duration (time interval), such as a timeout or a service’s response time:</p>

<h4 id="january-17-2021-6">January 17, 2021</h4>

<p>Variable delays in networks are not a law of nature, but simply the result of a cost/benefit trade-off.</p>

<h4 id="january-17-2021-7">January 17, 2021</h4>

<p>the downside of variable delays. Variable delays in networks are not a law of nature, but simply the result of a cost/benefit trade-off.</p>

<h4 id="january-17-2021-8">January 17, 2021</h4>

<p>using circuits for bursty data transfers wastes network capacity and makes transfers unnecessarily slow. By contrast, TCP dynamically adapts the rate of data transfer to the available network capacity.</p>

<h4 id="january-17-2021-9">January 17, 2021</h4>

<p>using circuits for bursty data transfers wastes network capacity and makes transfers unnecessarily slow. By contrast, TCP dynamically adapts the rate of data</p>

<h4 id="january-17-2021-10">January 17, 2021</h4>

<p>Note that a circuit in a telephone network is very different from a TCP connection: a circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas the packets of a TCP connection opportunistically use whatever network bandwidth is available. You can give TCP a variable-sized block of data (e.g., an email or a web page), and it will try to transfer it in the shortest time possible. While a TCP connection is idle, it doesn’t use any bandwidth.</p>

<h4 id="january-17-2021-11">January 17, 2021</h4>

<p>When you make a call over the telephone network, it establishes a circuit: a fixed, guaranteed amount of bandwidth is allocated for the call, along the entire route between the two callers. This circuit remains in place until the call ends</p>

<h4 id="january-17-2021-12">January 17, 2021</h4>

<p>Even better, rather than using configured constant timeouts, systems can continually measure response times and their variability (jitter), and automatically adjust timeouts according to the observed response time distribution. This can be done with a Phi Accrual failure detector [30], which is used for example in Akka and Cassandra [31]. TCP retransmission timeouts also work similarly [27].</p>

<h4 id="january-17-2021-13">January 17, 2021</h4>

<p>Some latency-sensitive applications, such as videoconferencing and Voice over IP (VoIP), use UDP rather than TCP. It’s a trade-off between reliability and variability of delays: as UDP does not perform flow control and does not retransmit lost packets, it avoids some of the reasons for variable network delays (although it is still susceptible to switch queues and scheduling delays). UDP is a good choice in situations where delayed data is worthless. For example, in a VoIP phone call, there probably isn’t enough time to retransmit a lost packet before its data is due to be played over the loudspeakers.</p>

<h4 id="january-17-2021-14">January 17, 2021</h4>

<p>Even if network faults are rare in your environment, the fact that faults can occur means that your software needs to be able to handle them. Whenever any communication happens over a network, it may fail—there is no way around it.</p>

<h4 id="january-17-2021-15">January 17, 2021</h4>

<p>It found that adding redundant networking gear doesn’t reduce faults as much as you might hope, since it doesn’t guard against human error (e.g., misconfigured switches), which is a major cause of outages.</p>

<h4 id="january-17-2021-16">January 17, 2021</h4>

<p>the distributed systems we focus on in this book are shared-nothing systems: i.e., a bunch of machines connected by a network. The network is the only way those machines can communicate—we assume that each machine has its own memory and disk, and one machine cannot access another machine’s memory or disk (except by making requests to a service over the network).</p>

<h4 id="january-17-2021-17">January 17, 2021</h4>

<p>In distributed systems, suspicion, pessimism, and paranoia pay off.</p>

<h4 id="january-16-2021">January 16, 2021</h4>

<p>Compared to serial execution, serializable snapshot isolation is not limited to the throughput of a single CPU core: FoundationDB distributes the detection of serialization conflicts across multiple machines, allowing it to scale to very high throughput. Even though data may be partitioned across multiple machines, transactions can read and write data in multiple partitions while ensuring serializable isolation</p>

<h4 id="january-16-2021-1">January 16, 2021</h4>

<p>Compared to two-phase locking, the big advantage of serializable snapshot isolation is that one transaction doesn’t need to block waiting for locks held by another transaction. Like under snapshot isolation, writers don’t block readers, and vice versa. This design principle makes query latency much more predictable and less variable. In particular, read-only queries can run on a consistent snapshot without requiring any locks, which is very appealing for read-heavy workloads.</p>

<h4 id="january-16-2021-2">January 16, 2021</h4>

<p>Optimistic in this context means that instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. When a transaction wants to commit, the database checks whether anything bad happened (i.e., whether isolation was violated); if so, the transaction is aborted and has to be retried. Only transactions that executed serializably are allowed to commit.</p>

<h4 id="january-16-2021-3">January 16, 2021</h4>

<p>Optimistic in this context means that instead of blocking if something potentially dangerous happens, transactions continue anyway, in the</p>

<h4 id="january-16-2021-4">January 16, 2021</h4>

<p>Are serializable isolation and good performance fundamentally at odds with each other? Perhaps not: an algorithm called serializable snapshot isolation (SSI) is very promising. It provides full serializability, but has only a small performance penalty compared to snapshot isolation.</p>

<h4 id="january-16-2021-5">January 16, 2021</h4>

<p>Index-range locks are not as precise as predicate locks would be (they may lock a bigger range of objects than is strictly necessary to maintain serializability), but since they have much lower overheads, they are a good compromise.</p>

<h4 id="january-15-2021">January 15, 2021</h4>

<p>Performance of two-phase locking The big downside of two-phase locking, and the reason why it hasn’t been used by everybody since the 1970s, is performance:</p>

<h4 id="january-15-2021-1">January 15, 2021</h4>

<p>Since so many locks are in use, it can happen quite easily that transaction A is stuck waiting for transaction B to release its lock, and vice versa. This situation is called deadlock. The database automatically detects deadlocks between transactions and aborts one of them so that the others can make progress. The aborted transaction needs to be retried by the application.</p>

<h4 id="january-15-2021-2">January 15, 2021</h4>

<p>In 2PL, writers don’t just block other writers; they also block readers and vice versa. Snapshot isolation has the mantra readers never block writers, and writers never block readers (see “Implementing snapshot isolation”), which captures this key difference between snapshot isolation and two-phase locking. On the other hand, because 2PL provides serializability, it protects against all the race conditions discussed earlier, including lost updates and write skew.</p>

<h4 id="january-15-2021-3">January 15, 2021</h4>

<p>Modern implementations of stored procedures have abandoned PL/SQL and use existing general-purpose programming languages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis uses Lua.</p>

<h4 id="january-15-2021-4">January 15, 2021</h4>

<p>Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, serially, without any concurrency. Thus, the database guarantees that if the transactions behave correctly when run individually, they continue to be correct when run concurrently—in other words, the database prevents all possible race conditions. But if serializable isolation is so much better than the mess of weak isolation levels, then why isn’t everyone using it? To answer this question, we need to look at the options for implementing serializability, and how they perform.</p>

<h4 id="january-15-2021-5">January 15, 2021</h4>

<p>Serializable isolation is usually regarded as the strongest isolation level. It guarantees that even</p>

<h4 id="january-14-2021">January 14, 2021</h4>

<p>This effect, where a write in one transaction changes the result of a search query in another transaction, is called a phantom [3]. Snapshot isolation avoids phantoms in read-only queries, but in read-write transactions like the examples we discussed, phantoms can lead to particularly tricky cases of write skew.</p>

<h4 id="january-14-2021-1">January 14, 2021</h4>

<p>In the special case where different transactions update the same object, you get a dirty write or lost update anomaly (depending on the timing).</p>

<h4 id="january-14-2021-2">January 14, 2021</h4>

<p>You can think of write skew as a generalization of the lost update problem. Write skew can occur if two transactions read the same objects, and then update some of those objects (different transactions may update different objects).</p>

<h4 id="january-14-2021-3">January 14, 2021</h4>

<p>An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.</p>

<h4 id="january-14-2021-4">January 14, 2021</h4>

<p>An alternative is to allow them to execute in parallel and, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle. An advantage of this approach is that</p>

<h4 id="january-14-2021-5">January 14, 2021</h4>

<p>The FOR UPDATE clause indicates that the database should take a lock on all rows returned by this query.</p>

<h4 id="january-14-2021-6">January 14, 2021</h4>

<p>database’s built-in atomic operations don’t provide the necessary functionality, is for the application to explicitly lock objects that are going to be updated.</p>

<h4 id="january-14-2021-7">January 14, 2021</h4>

<p>Atomic operations are usually implemented by taking an exclusive lock on the object when it is read so that no other transaction can read it until the update has been applied. This technique is sometimes known as cursor stability</p>

<h4 id="january-14-2021-8">January 14, 2021</h4>

<p>The lost update problem can occur if an application reads some value from the database, modifies it, and writes back the modified value (a read-modify-write cycle). If two transactions do this concurrently, one of the modifications can be lost, because the second write does not include the first modification.</p>

<h4 id="january-12-2021">January 12, 2021</h4>

<p>The database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as multi-version concurrency control (MVCC).</p>

<h4 id="january-12-2021-1">January 12, 2021</h4>

<p>saw for preventing dirty reads in Figure 7-4. The database must potentially keep several different committed versions of an object, because various in-progress transactions may need to see the state of the database at different points in time. Because it maintains several versions of an object side by side, this technique is known as multi-version concurrency control (MVCC).</p>

<h4 id="january-12-2021-2">January 12, 2021</h4>

<p>snapshot isolation is readers never block writers, and writers never block readers.</p>

<h4 id="january-12-2021-3">January 12, 2021</h4>

<p>provide a safe place where data can be stored without fear of losing it.</p>

<h4 id="january-12-2021-4">January 12, 2021</h4>

<p>Isolation in the sense of ACID means that concurrently executing transactions are isolated from each other: they cannot step on each other’s toes.</p>

<h4 id="january-10-2021">January 10, 2021</h4>

<p>Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application. The application may rely on the database’s atomicity and isolation properties in order to achieve consistency, but it’s not up to the database alone. Thus, the letter C doesn’t really belong in ACID.i</p>

<h4 id="january-10-2021-1">January 10, 2021</h4>

<p>database only stores it.) Atomicity, isolation, and durability are properties of the database, whereas consistency (in the ACID sense) is a property of the application. The application may rely on the database’s atomicity and isolation properties in order to achieve consistency, but it’s not up to the database alone. Thus,</p>

<h4 id="january-10-2021-2">January 10, 2021</h4>

<p>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Perhaps abortability would have been a better term than atomicity, but we will stick with atomicity since that’s the usual word.</p>

<h4 id="january-10-2021-3">January 10, 2021</h4>

<p>The ability to abort a transaction on error and have all writes from that transaction discarded is the defining feature of ACID atomicity. Perhaps abortability would have been a better term than atomicity, but we will stick</p>

<h4 id="january-10-2021-4">January 10, 2021</h4>

<p>of ambiguity around the meaning of isolation [8]. The high-level idea is sound, but the devil is in the details. Today, when a system claims to be “ACID compliant,” it’s unclear what guarantees you can actually expect. ACID has unfortunately become mostly a marketing term.</p>

<h4 id="january-10-2021-5">January 10, 2021</h4>

<p>ACID, which stands for Atomicity, Consistency, Isolation, and Durability.</p>

<h4 id="january-10-2021-6">January 10, 2021</h4>

<p>Hash partitioning, where a hash function is applied to each key, and a partition owns a range of hashes. This method destroys the ordering of keys, making range queries inefficient, but may distribute load more evenly. When partitioning by hash, it is common to create a fixed number of partitions in advance, to assign several partitions to each node, and to move entire partitions from one node to another when nodes are added or removed. Dynamic partitioning can also be used.</p>

<h4 id="january-10-2021-7">January 10, 2021</h4>

<p>Key range partitioning, where keys are sorted, and a partition owns all the keys from some minimum up to some maximum. Sorting has the advantage that efficient range queries are possible, but there is a risk of hot spots if the application often accesses keys that are close together in the sorted order. In this approach, partitions are typically rebalanced dynamically by splitting the range into two subranges when a partition gets too big.</p>

<h4 id="january-10-2021-8">January 10, 2021</h4>

<p>The goal of partitioning is to spread the data and query load evenly across multiple machines, avoiding hot spots (nodes with disproportionately high load). This requires choosing a partitioning scheme that is appropriate to your data, and rebalancing the partitions when nodes are added to or removed from the cluster.</p>

<h4 id="january-10-2021-9">January 10, 2021</h4>

<p>datastores. However, massively parallel processing (MPP) relational database products, often used for analytics, are much more sophisticated in the types of queries they support. A typical data warehouse query contains several join, filtering, grouping, and aggregation operations. The MPP query optimizer breaks this complex query into a number of execution stages and partitions, many of which can be executed in parallel on different nodes of the database cluster.</p>

<h4 id="january-10-2021-10">January 10, 2021</h4>

<p>HBase, SolrCloud, and Kafka also use ZooKeeper to track partition assignment. MongoDB has a similar architecture, but it relies on its own config server implementation and mongos daemons as the routing tier.</p>

<h4 id="january-10-2021-11">January 10, 2021</h4>

<p>Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.</p>

<h4 id="january-10-2021-12">January 10, 2021</h4>

<p>can subscribe to this information in ZooKeeper. Whenever a partition changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information</p>

<h4 id="january-10-2021-13">January 10, 2021</h4>

<p>Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata, as illustrated in Figure 6-8. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes.</p>

<h4 id="january-09-2021">January 09, 2021</h4>

<p>when you increase the number of nodes, the partitions become smaller again. Since a larger data volume generally requires a larger number of nodes to store, this approach also keeps the size of each partition fairly stable.</p>

<h4 id="january-09-2021-1">January 09, 2021</h4>

<p>with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes. A third option, used by Cassandra and Ketama, is to make the number of partitions proportional to the number of nodes—in other words, to have a fixed number of partitions per node</p>

<h4 id="january-09-2021-2">January 09, 2021</h4>

<p>with a fixed number of partitions, the size of each partition is proportional to the size of the dataset. In both of these cases, the number of partitions is independent of the number of nodes.</p>

<h4 id="january-09-2021-3">January 09, 2021</h4>

<p>Dynamic partitioning is not only suitable for key range–partitioned data, but can equally well be used with hash-partitioned data. MongoDB since version 2.4 supports both key-range and hash partitioning, and it splits partitions dynamically in either case.</p>

<h4 id="january-09-2021-4">January 09, 2021</h4>

<p>the number of partitions configured at the outset is the maximum number of nodes you can have, so you need to choose it high enough to accommodate future growth. However, each partition also has management overhead, so it’s counterproductive to choose too high a number.</p>

<h4 id="january-09-2021-5">January 09, 2021</h4>

<p>The process of moving load from one node in the cluster to another is called rebalancing.</p>

<h4 id="january-09-2021-6">January 09, 2021</h4>

<p>The advantage of a global (term-partitioned) index over a document-partitioned index is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. However, the downside of a global index is that writes are slower and more complicated, because a write to a single document may now affect multiple partitions of the index</p>

<h4 id="january-09-2021-7">January 09, 2021</h4>

<p>h). We call this kind of index term-partitioned, because the term we’re looking for determines the partition of the index.</p>

<h4 id="january-09-2021-8">January 09, 2021</h4>

<p>This approach to querying a partitioned database is sometimes known as scatter/gather, and it can make read queries on secondary indexes quite expensive. Even if you query the partitions in parallel, scatter/gather is prone to tail latency amplification (see “Percentiles in Practice”). Nevertheless, it is widely used:</p>

<h4 id="january-09-2021-9">January 09, 2021</h4>

<p>if you want to search for red cars, you need to send the query to all partitions, and combine all the results you get back.</p>

<h4 id="january-09-2021-10">January 09, 2021</h4>

<p>if you want to search for red cars, you need to send the query to all partitions, and combine all the results</p>

<h4 id="january-09-2021-11">January 09, 2021</h4>

<p>In this indexing approach, each partition is completely separate: each partition maintains its own secondary indexes, covering only the documents in that partition. It doesn’t care what data is stored in other partitions. Whenever you need to write to the database—to add, remove, or update a document—you only need to deal with the partition that contains the document ID that you are writing. For that reason, a document-partitioned index is also known as a local index (as opposed to a global index, described in the next section).</p>

<h4 id="january-09-2021-12">January 09, 2021</h4>

<p>The problem with secondary indexes is that they don’t map neatly to partitions. There are two main approaches to partitioning a database with secondary indexes: document-based partitioning and term-based partitioning.</p>

<h4 id="january-09-2021-13">January 09, 2021</h4>

<p>secondary indexes are the raison d’être of search servers such as Solr and Elasticsearch.</p>

<h4 id="january-09-2021-14">January 09, 2021</h4>

<p>Unfortunately however, by using the hash of the key for partitioning we lose a nice property of key-range partitioning: the ability to do efficient range queries.</p>

<h4 id="january-09-2021-15">January 09, 2021</h4>

<p>A partition with disproportionately high load is called a hot spot.</p>

<h4 id="january-09-2021-16">January 09, 2021</h4>

<p>If the partitioning is unfair, so that some partitions have more data or queries than others, we call it skewed.</p>

<h4 id="january-08-2021">January 08, 2021</h4>

<p>For defining concurrency, exact time doesn’t matter: we simply call two operations concurrent if they are both unaware of each other, regardless of the physical time at which they occurred.</p>

<h4 id="january-08-2021-1">January 08, 2021</h4>

<p>An operation A happens before another operation B if B knows about A, or depends on A, or builds upon A in some way. Whether one operation happens before another operation is the key to defining what concurrency means. In fact, we can simply say that two operations are concurrent if neither happens before the other (i.e., neither knows about the other) [54]. Thus, whenever you have two operations A and B, there are three possibilities: either A happened before B, or B happened before A, or A and B are concurrent.</p>

<h4 id="january-07-2021">January 07, 2021</h4>

<p>This is unfortunately not yet common practice, but it would be good to include staleness measurements in the standard set of metrics for databases. Eventual consistency is a deliberately vague guarantee, but for operability it’s important to be able to quantify “eventual.”</p>

<h4 id="january-07-2021-1">January 07, 2021</h4>

<p>Even if your application can tolerate stale reads, you need to be aware of the health of your replication. If it falls behind significantly, it should alert you so that you can investigate the cause (for example, a problem in the network or an overloaded node).</p>

<h4 id="january-06-2021">January 06, 2021</h4>

<p>More generally, if there are n replicas, every write must be confirmed by w nodes to be considered successful, and we must query at least r nodes for each read. (In our example, n = 3, w = 2, r = 2.) As long as w + r &gt; n, we expect to get an up-to-date value when reading, because at least one of the r nodes we’re reading from must be up to date. Reads and writes that obey these r and w values are called quorum reads and writes [44].vii You can think of r and w as the minimum number of votes required for the read or write to be valid. In Dynamo-style databases, the parameters n, w, and r are typically configurable. A common choice is to make n an odd number (typically 3 or 5) and to set w = r = (n + 1) / 2 (rounded up). However, you can vary the numbers as you see fit. For example, a workload with few writes and many reads may benefit from setting w = n and r = 1. This makes reads faster, but has the disadvantage that just one failed node causes all database writes to fail.</p>

<h4 id="january-06-2021-1">January 06, 2021</h4>

<p>Some data storage systems take a different approach, abandoning the concept of a leader and allowing any replica to directly accept writes from clients. Some of the earliest replicated data systems were leaderless [1, 44], but the idea was mostly forgotten during the era of dominance of relational databases. It once again became a fashionable architecture for databases after Amazon used it for its in-house Dynamo system [37].vi Riak, Cassandra, and Voldemort are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as Dynamo-style.</p>

<h4 id="january-06-2021-2">January 06, 2021</h4>

<p>It once again became a fashionable architecture for databases after Amazon used it for its in-house Dynamo system [37].vi Riak, Cassandra, and Voldemort are open source datastores with leaderless replication models inspired by Dynamo, so this kind of database is also known as Dynamo-style. In some leaderless implementations, the client</p>

<h4 id="january-06-2021-3">January 06, 2021</h4>

<p>Operational transformation [42] is the conflict resolution algorithm behind collaborative editing applications such as Etherpad [30] and Google Docs [31]. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute a text document.</p>

<h4 id="january-06-2021-4">January 06, 2021</h4>

<p>Operational transformation [42] is the conflict resolution algorithm behind collaborative editing applications such as Etherpad [30] and Google Docs [31]. It was designed particularly for concurrent editing of an ordered list of items, such as the list of characters that constitute</p>

<h4 id="january-06-2021-5">January 06, 2021</h4>

<p>Give each write a unique ID (e.g., a timestamp, a long random number, a UUID, or a hash of the key and value), pick the write with the highest ID as the winner, and throw away the other writes. If a timestamp is used, this technique is known as last write wins (LWW). Although this approach is popular, it is dangerously prone to data loss</p>

<h4 id="january-06-2021-6">January 06, 2021</h4>

<p>As the rich history of broken calendar sync implementations demonstrates, multi-leader replication is a tricky thing to get right.</p>

<h4 id="january-06-2021-7">January 06, 2021</h4>

<p>You need to be able to see your meetings (make read requests) and enter new meetings (make write requests) at any time, regardless of whether your device currently has an internet connection. If you make any changes while you are offline, they need to be synced with a server and your other devices when the device is next online.</p>

<h4 id="january-06-2021-8">January 06, 2021</h4>

<p>Single-node transactions have existed for a long time. However, in the move to distributed (replicated and partitioned) databases, many systems have abandoned them, claiming that transactions are too expensive in terms of performance and availability, and asserting that eventual consistency is inevitable in a scalable system.</p>

<h4 id="january-06-2021-9">January 06, 2021</h4>

<p>databases, which we will discuss in Chapter 6. If the database always applies writes in the same order, reads always see a consistent prefix, so this anomaly cannot happen. However, in many distributed databases, different partitions operate independently, so there is no global ordering of writes: when a user reads from the database, they may see some parts of the database in an older state and some in a newer state.</p>

<h4 id="january-06-2021-10">January 06, 2021</h4>

<p>When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward—i.e., they will not read older data after having previously read newer data.</p>

<h4 id="january-06-2021-11">January 06, 2021</h4>

<p>In this situation, we need read-after-write consistency, also known as read-your-writes consistency [24]. This is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves. It makes no promises about other users: other users’ updates may not be visible until some later time. However, it reassures the user that their own input has been saved correctly.</p>

<h4 id="january-06-2021-12">January 06, 2021</h4>

<p>Logical (row-based) log replication An alternative is to use different log formats for replication and for the storage engine, which allows the replication log to be decoupled from the storage engine internals. This kind of replication log is called a logical log, to distinguish it from the storage engine’s (physical) data</p>

<h4 id="january-06-2021-13">January 06, 2021</h4>

<p>disadvantage is that the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine. If the database changes its storage format from one version to another, it is typically not possible to run different versions of the database software on the leader and the followers.</p>

<h4 id="january-06-2021-14">January 06, 2021</h4>

<p>disadvantage is that the log describes the data on a very low level: a WAL contains details of which bytes were changed in which disk blocks. This makes replication closely coupled to the storage engine. If the database changes its storage format from one version to another, it is typically not</p>

<h4 id="january-06-2021-15">January 06, 2021</h4>

<p>We can use the exact same log to build a replica on another node: besides writing the log to disk, the leader also sends it across the network to its followers. When the follower processes this log, it builds a copy of the exact same data structures as found on the leader.</p>

<h4 id="january-06-2021-16">January 06, 2021</h4>

<p>However, because there are so many edge cases, other replication methods are now generally preferred. Statement-based replication was used in MySQL before version 5.1. It is still sometimes used today, as it is quite compact, but by default MySQL now switches to row-based replication (discussed shortly) if there is any nondeterminism in a statement.</p>

<h4 id="january-05-2021">January 05, 2021</h4>

<p>All of the difficulty in replication lies in handling changes to replicated data, and that’s what this chapter is about. We will discuss three popular algorithms for replicating changes between nodes: single-leader, multi-leader, and leaderless replication.</p>

<h4 id="january-04-2021">January 04, 2021</h4>

<p>We can conclude that with a bit of care, backward/forward compatibility and rolling upgrades are quite achievable. May your application’s evolution be rapid and your deployments be frequent.</p>

<h4 id="january-04-2021-1">January 04, 2021</h4>

<p>A distributed actor framework essentially integrates a message broker and the actor programming model into a single framework.</p>

<h4 id="january-04-2021-2">January 04, 2021</h4>

<p>More recently, open source implementations such as RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache Kafka have become popular. We will compare them in more detail in Chapter 11.</p>

<h4 id="january-04-2021-3">January 04, 2021</h4>

<p>There is no agreement on how API versioning should work (i.e., how a client can indicate which version of the API it wants to use [48]). For RESTful APIs, common approaches are to use a version number in the URL or in the HTTP Accept header.</p>

<h4 id="january-04-2021-4">January 04, 2021</h4>

<p>The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it uses:</p>

<h4 id="january-04-2021-5">January 04, 2021</h4>

<p>The backward and forward compatibility properties of an RPC scheme are inherited from whatever encoding it</p>

<h4 id="january-04-2021-6">January 04, 2021</h4>

<p>Custom RPC protocols with a binary encoding format can achieve better performance than something generic like JSON over REST. However, a RESTful API has other significant advantages: it is good for experimentation and debugging (you can simply make requests to it using a web browser or the command-line tool curl, without any code generation or software installation), it is supported by all mainstream programming languages and platforms, and there is a vast ecosystem of tools available (servers, caches, load balancers, proxies, firewalls, monitoring, debugging tools, testing tools, etc.). For these reasons, REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organization, typically within the same datacenter.</p>

<h4 id="january-04-2021-7">January 04, 2021</h4>

<p>A key design goal of a service-oriented/microservices architecture is to make the application easier to change and maintain by making services independently deployable and evolvable.</p>

<h4 id="january-04-2021-8">January 04, 2021</h4>

<p>A key design goal of a service-oriented/microservices architecture is to make the application easier to change and maintain by making services</p>

<h4 id="january-04-2021-9">January 04, 2021</h4>

<p>one service makes a request to another when it requires some functionality or data from that other service. This way of building applications has traditionally been called a service-oriented architecture (SOA), more recently refined and rebranded as microservices architecture</p>

<h4 id="january-02-2021">January 02, 2021</h4>

<p>Apache Thrift [15] and Protocol Buffers (protobuf) [16] are binary encoding libraries that are based on the same principle. Protocol Buffers was originally developed at Google, Thrift was originally developed at Facebook, and both were made open source in 2007–08 [17].</p>

<h4 id="january-02-2021-1">January 02, 2021</h4>

<p>JSON is less verbose than XML, but both still use a lot of space compared to binary formats. This observation led to the development of a profusion of binary encodings for JSON</p>

<h4 id="january-02-2021-2">January 02, 2021</h4>

<p>Use of XML schemas is fairly widespread, but many JSON-based tools don’t bother using schemas. Since the correct interpretation of data (such as numbers and binary strings) depends on information in the schema, applications that don’t use XML/JSON schemas need to potentially hardcode the appropriate encoding/decoding logic instead.</p>

<h4 id="january-02-2021-3">January 02, 2021</h4>

<p>Since the correct interpretation of data (such as numbers and binary strings) depends on information in the schema, applications that don’t use XML/JSON schemas need to potentially hardcode the appropriate</p>

<h4 id="january-02-2021-4">January 02, 2021</h4>

<p>JSON’s popularity is mainly due to its built-in support in web browsers (by virtue of being a subset of JavaScript) and simplicity relative to XML.</p>

<h4 id="january-02-2021-5">January 02, 2021</h4>

<p>why analytic workloads are so different from OLTP: when your queries require sequentially scanning across a large number of rows, indexes are much less relevant. Instead it becomes important to encode data very compactly, to minimize the amount of data that the query needs to read from disk. We discussed how column-oriented storage helps achieve this goal.</p>

<h4 id="january-02-2021-6">January 02, 2021</h4>

<p>The update-in-place school, which treats the disk as a set of fixed-size pages that can be overwritten. B-trees are the biggest example of this philosophy, being used in all major relational databases and also many nonrelational ones.</p>

<h4 id="january-02-2021-7">January 02, 2021</h4>

<p>The log-structured school, which only permits appending to files and deleting obsolete files, but never updates a file that has been written.</p>

<h4 id="january-02-2021-8">January 02, 2021</h4>

<p>we saw that storage engines fall into two broad categories: those optimized for transaction processing (OLTP), and those optimized for analytics (OLAP).</p>

<h4 id="january-01-2021">January 01, 2021</h4>

<p>The idea behind column-oriented storage is simple: don’t store all the values from one row together, but store all the values from each column together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in that query, which can save a lot of work.</p>

<h4 id="january-01-2021-1">January 01, 2021</h4>

<p>The name “star schema” comes from the fact that when the table relationships are visualized, the fact table is in the middle, surrounded by its dimension tables; the connections to these tables are like the rays of a star.</p>

<h4 id="january-01-2021-2">January 01, 2021</h4>

<p>between sales on holidays and non-holidays. The name</p>

<h4 id="january-01-2021-3">January 01, 2021</h4>

<p>Other columns in the fact table are foreign key references to other tables, called dimension tables. As each row in the fact table represents an event, the dimensions represent the who, what, where, when, how, and why of the event.</p>

<h4 id="january-01-2021-4">January 01, 2021</h4>

<p>process of getting data into the warehouse is known as Extract–Transform–Load (ETL) and is illustrated in Figure 3-8.</p>

<h4 id="january-01-2021-5">January 01, 2021</h4>

<p>The data warehouse contains a read-only copy of the data in all the various OLTP systems in the company. Data is extracted from OLTP databases (using either a periodic data dump or a continuous stream of updates), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse.</p>

<h4 id="january-01-2021-6">January 01, 2021</h4>

<p>The log-structured indexes we have discussed so far are gaining acceptance, but they are not the most common type of index. The most widely used indexing structure is quite different: the B-tree.</p>

<h4 id="january-01-2021-7">January 01, 2021</h4>

<p>The log-structured indexes we have discussed so far are gaining acceptance, but they are not the most common type of index. The most widely used indexing</p>

<h4 id="december-30-2020">December 30, 2020</h4>

<p>Facebook maintains a single graph with many different types of vertices and edges: vertices represent people, locations, events, checkins, and comments made by users; edges indicate which people are friends with each other, which checkin happened in which location, who commented on which post, who attended which event, and so on [35].</p>

<h4 id="december-29-2020">December 29, 2020</h4>

<p>On updates to a document, the entire document usually needs to be rewritten—only modifications that don’t change the encoded size of a document can easily be performed in place [19]. For these reasons, it is generally recommended that you keep documents fairly small and avoid writes that increase the size of a document [9]. These performance limitations significantly reduce the set of situations in which document databases are useful.</p>

<h4 id="december-29-2020-1">December 29, 2020</h4>

<p>The database typically needs to load the entire document, even if you access only a small portion of it, which can be wasteful on large documents.</p>

<h4 id="december-29-2020-2">December 29, 2020</h4>

<p>There are many differences to consider when comparing relational databases to document databases, including their fault-tolerance properties (see Chapter 5) and handling of concurrency (see Chapter 7). In this chapter, we will concentrate only on the differences in the data model.</p>

<h4 id="december-29-2020-3">December 29, 2020</h4>

<p>Most application development today is done in object-oriented programming languages, which leads to a common criticism of the SQL data model: if data is stored in relational tables, an awkward translation layer is required between the objects in the application code and the database model of tables, rows, and columns.</p>

<h4 id="december-29-2020-4">December 29, 2020</h4>

<p>since the data model has such a profound effect on what the software above it can and can’t do, it’s important to choose one that is appropriate to the application.</p>

<h4 id="december-27-2020">December 27, 2020</h4>

<p>The response time is what the client sees: besides the actual time to process the request (the service time), it includes network delays and queueing delays.</p>

<h4 id="december-27-2020-1">December 27, 2020</h4>

<p>Latency is the duration that a request is waiting to be handled—during which it is latent, awaiting service [17].</p>

<h4 id="december-27-2020-2">December 27, 2020</h4>

<p>there is a move toward systems that can tolerate the loss of entire machines, by using software fault-tolerance techniques in preference or in addition to hardware redundancy.</p>

<h4 id="december-23-2020">December 23, 2020</h4>

<p>Maintainability Over time, many different people will work on the system (engineering and operations, both maintaining current behavior and adapting the system to new use cases), and they should all be able to work on it productively.</p>

<h4 id="december-23-2020-1">December 23, 2020</h4>

<p>Scalability As the system grows (in data volume, traffic volume, or complexity), there should be reasonable ways of dealing with that growth.</p>

<h4 id="december-23-2020-2">December 23, 2020</h4>

<p>Reliability The system should continue to work correctly (performing the correct function at the desired level of performance) even in the face of adversity (hardware or software faults, and even human error).</p>

<h4 id="december-23-2020-3">December 23, 2020</h4>

<p>If you are designing a data system or service, a lot of tricky questions arise. How do you ensure that the data remains correct and complete, even when things go wrong internally? How do you provide consistently good performance to clients, even when parts of your system are degraded? How do you scale to handle an increase in load? What does a good API for the service look like?</p>


        
      </section>

      <footer class="page__meta">
      <!-- Begin Mailchimp Signup Form -->
        <link href="//cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css">
        <div id="mc_embed_signup">
        <form action="https://pallares.us1.list-manage.com/subscribe/post?u=46b0c297119e6421f6fe0d3f3&amp;id=6215063107" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
            <div id="mc_embed_signup_scroll">
          <label for="mce-EMAIL">Don't miss next post, get it in your inbox!</label>
          <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="email address" required>
            <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
            <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_46b0c297119e6421f6fe0d3f3_6215063107" tabindex="-1" value=""></div>
            <div class="clear"><input type="submit" value="Subscribe" name="subscribe" id="mc-embedded-subscribe" class="button"></div>
            </div>
        </form>
        </div>
      <!--End mc_embed_signup-->
        
        




        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> </strong> <time datetime="2021-03-02T00:00:00+01:00">March 02, 2021</time></p>
        
      </footer>

      

      


    </div>

    
  </article>

  
  
</div>

    <div class="page__footer">
      <footer>
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
    
      <li><a href="https://twitter.com/juanpallares"><i class="fa fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
    
    
    
      <li><a href="http://github.com/jpallares"><i class="fa fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://juan.pallares.me/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> </a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Juan Pallarès Garbí.  <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>
      </footer>
    </div>

    <script src="https://juan.pallares.me/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54458401-1', 'auto');
  ga('send', 'pageview');
</script>





    <!-- start custom footer snippets -->

<!-- end custom footer snippets -->

  </body>
</html>

